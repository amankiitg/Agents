{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amankiitg/Agents/blob/main/Agentic_RAG_using_LlamaIndex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1ME8m2EE3c4"
      },
      "source": [
        "# Agentic RAG in LlamaIndex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 0: Loading libraries"
      ],
      "metadata": {
        "id": "-M4zgrIYtzEW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gpk5DaC8E3c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d941c2ee-aff3-43b9-8f5f-f13b6367c556"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.7/309.7 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install llama-index llama-index-vector-stores-chroma llama-index-llms-huggingface-api llama-index-embeddings-huggingface -U -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets\n",
        "!pip install --upgrade huggingface-hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEgI0tKgK0df",
        "outputId": "aeabbf2f-c5ea-4afa-ba40-496489a22e48"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (0.33.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-callbacks-arize-phoenix arize-phoenix\n",
        "!pip install llama-index-tools-arxiv llama-index-tools-wikipedia duckduckgo-search\n",
        "!pip install llama-index-tools-brave-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeqxN4RPK-uI",
        "outputId": "ef54309f-0e3c-4fe2-863b-7921cf713ad5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-callbacks-arize-phoenix\n",
            "  Downloading llama_index_callbacks_arize_phoenix-0.5.1-py3-none-any.whl.metadata (744 bytes)\n",
            "Collecting arize-phoenix\n",
            "  Downloading arize_phoenix-11.8.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-callbacks-arize-phoenix) (0.12.50)\n",
            "Collecting openinference-instrumentation-llama-index>=4.1.0 (from llama-index-callbacks-arize-phoenix)\n",
            "  Downloading openinference_instrumentation_llama_index-4.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting aioitertools (from arize-phoenix)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.21.0)\n",
            "Collecting alembic<2,>=1.3.0 (from arize-phoenix)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting arize-phoenix-client (from arize-phoenix)\n",
            "  Downloading arize_phoenix_client-1.13.2-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting arize-phoenix-evals>=0.20.6 (from arize-phoenix)\n",
            "  Downloading arize_phoenix_evals-0.23.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting arize-phoenix-otel>=0.10.3 (from arize-phoenix)\n",
            "  Downloading arize_phoenix_otel-0.12.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting authlib (from arize-phoenix)\n",
            "  Downloading authlib-1.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (5.5.2)\n",
            "Collecting email-validator (from arize-phoenix)\n",
            "  Downloading email_validator-2.2.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.116.1)\n",
            "Requirement already satisfied: grpc-interceptor in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.15.4)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.73.1)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.28.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (3.1.6)\n",
            "Requirement already satisfied: numpy!=2.0.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.0.2)\n",
            "Collecting openinference-instrumentation>=0.1.32 (from arize-phoenix)\n",
            "  Downloading openinference_instrumentation-0.1.35-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting openinference-semantic-conventions>=0.1.20 (from arize-phoenix)\n",
            "  Downloading openinference_semantic_conventions-0.1.21-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting opentelemetry-exporter-otlp (from arize-phoenix)\n",
            "  Downloading opentelemetry_exporter_otlp-1.35.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-proto>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-sdk in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.56b0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.22.1)\n",
            "Requirement already satisfied: protobuf>=4.25.8 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (5.29.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (18.1.0)\n",
            "Requirement already satisfied: pydantic>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.11.7)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (2.9.0.post0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.0.20)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (2.0.41)\n",
            "Collecting sqlean-py>=3.45.1 (from arize-phoenix)\n",
            "  Downloading sqlean_py-3.49.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: starlette in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.47.1)\n",
            "Collecting strawberry-graphql==0.270.1 (from arize-phoenix)\n",
            "  Downloading strawberry_graphql-0.270.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (4.14.1)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (0.35.0)\n",
            "Requirement already satisfied: wrapt>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from arize-phoenix) (1.17.2)\n",
            "Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql==0.270.1->arize-phoenix)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=23 in /usr/local/lib/python3.11/dist-packages (from strawberry-graphql==0.270.1->arize-phoenix) (25.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic<2,>=1.3.0->arize-phoenix) (1.1.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.11.15)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2025.3.0)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.9.1)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (11.2.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (4.3.8)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (80.9.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.9.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.9.0)\n",
            "Requirement already satisfied: opentelemetry-api in /usr/local/lib/python3.11/dist-packages (from openinference-instrumentation>=0.1.32->arize-phoenix) (1.35.0)\n",
            "Collecting opentelemetry-instrumentation (from openinference-instrumentation-llama-index>=4.1.0->llama-index-callbacks-arize-phoenix)\n",
            "  Downloading opentelemetry_instrumentation-0.56b0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->arize-phoenix) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->arize-phoenix) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.1.0->arize-phoenix) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.1.0->arize-phoenix) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.1.0->arize-phoenix) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->arize-phoenix) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=2.0.4->sqlalchemy[asyncio]<3,>=2.0.4->arize-phoenix) (3.2.3)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib->arize-phoenix) (43.0.3)\n",
            "Collecting dnspython>=2.0.0 (from email-validator->arize-phoenix)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from email-validator->arize-phoenix) (3.10)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette->arize-phoenix) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx->arize-phoenix) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->arize-phoenix) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->arize-phoenix) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->arize-phoenix) (3.0.2)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp->arize-phoenix) (1.35.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http==1.35.0 (from opentelemetry-exporter-otlp->arize-phoenix)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.35.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.35.0->opentelemetry-exporter-otlp->arize-phoenix) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc==1.35.0->opentelemetry-exporter-otlp->arize-phoenix) (1.35.0)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api->openinference-instrumentation>=0.1.32->arize-phoenix) (8.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->arize-phoenix) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->arize-phoenix) (3.6.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->arize-phoenix) (8.2.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.20.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette->arize-phoenix) (1.3.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.7.3)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2024.11.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (2.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib->arize-phoenix) (1.17.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (3.26.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib->arize-phoenix) (2.22)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api->openinference-instrumentation>=0.1.32->arize-phoenix) (3.23.0)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-callbacks-arize-phoenix) (0.4.6)\n",
            "Downloading llama_index_callbacks_arize_phoenix-0.5.1-py3-none-any.whl (3.1 kB)\n",
            "Downloading arize_phoenix-11.8.0-py3-none-any.whl (4.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading strawberry_graphql-0.270.1-py3-none-any.whl (301 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.2/301.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_evals-0.23.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arize_phoenix_otel-0.12.1-py3-none-any.whl (13 kB)\n",
            "Downloading openinference_instrumentation-0.1.35-py3-none-any.whl (28 kB)\n",
            "Downloading openinference_instrumentation_llama_index-4.3.1-py3-none-any.whl (28 kB)\n",
            "Downloading openinference_semantic_conventions-0.1.21-py3-none-any.whl (10 kB)\n",
            "Downloading sqlean_py-3.49.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading arize_phoenix_client-1.13.2-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.4/99.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading authlib-1.6.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading email_validator-2.2.0-py3-none-any.whl (33 kB)\n",
            "Downloading opentelemetry_exporter_otlp-1.35.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.35.0-py3-none-any.whl (18 kB)\n",
            "Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation-0.56b0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: sqlean-py, openinference-semantic-conventions, graphql-core, dnspython, aioitertools, strawberry-graphql, email-validator, alembic, authlib, arize-phoenix-evals, opentelemetry-instrumentation, opentelemetry-exporter-otlp-proto-http, openinference-instrumentation, opentelemetry-exporter-otlp, openinference-instrumentation-llama-index, llama-index-callbacks-arize-phoenix, arize-phoenix-otel, arize-phoenix-client, arize-phoenix\n",
            "Successfully installed aioitertools-0.12.0 alembic-1.16.4 arize-phoenix-11.8.0 arize-phoenix-client-1.13.2 arize-phoenix-evals-0.23.0 arize-phoenix-otel-0.12.1 authlib-1.6.0 dnspython-2.7.0 email-validator-2.2.0 graphql-core-3.2.6 llama-index-callbacks-arize-phoenix-0.5.1 openinference-instrumentation-0.1.35 openinference-instrumentation-llama-index-4.3.1 openinference-semantic-conventions-0.1.21 opentelemetry-exporter-otlp-1.35.0 opentelemetry-exporter-otlp-proto-http-1.35.0 opentelemetry-instrumentation-0.56b0 sqlean-py-3.49.1 strawberry-graphql-0.270.1\n",
            "Collecting llama-index-tools-arxiv\n",
            "  Downloading llama_index_tools_arxiv-0.3.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting llama-index-tools-wikipedia\n",
            "  Downloading llama_index_tools_wikipedia-0.3.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.1.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting arxiv<3.0.0,>=2.1.0 (from llama-index-tools-arxiv)\n",
            "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-tools-arxiv) (0.12.50)\n",
            "Collecting wikipedia<2.0,>=1.4 (from llama-index-tools-wikipedia)\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.2.1)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.4.0)\n",
            "Collecting feedparser~=6.0.10 (from arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.11/dist-packages (from arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (2.32.3)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (11.2.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (6.0.2)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.17.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia<2.0,>=1.4->llama-index-tools-wikipedia) (4.13.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.1.6)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests~=2.32.0->arxiv<3.0.0,>=2.1.0->llama-index-tools-arxiv) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia<2.0,>=1.4->llama-index-tools-wikipedia) (2.7)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-arxiv) (3.0.2)\n",
            "Downloading llama_index_tools_arxiv-0.3.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading llama_index_tools_wikipedia-0.3.1-py3-none-any.whl (3.5 kB)\n",
            "Downloading duckduckgo_search-8.1.1-py3-none-any.whl (18 kB)\n",
            "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
            "Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia, sgmllib3k\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11757 sha256=eba83b8e85b29ac51d4b491a6b5a48f9900b3c48ed6f4438b9d656f6c7ee14c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=b36ec997d23948b4d2e93466ae8bcd39f8cb475bd59db3c21753146ac5207cdc\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built wikipedia sgmllib3k\n",
            "Installing collected packages: sgmllib3k, primp, feedparser, wikipedia, duckduckgo-search, arxiv, llama-index-tools-wikipedia, llama-index-tools-arxiv\n",
            "Successfully installed arxiv-2.2.0 duckduckgo-search-8.1.1 feedparser-6.0.11 llama-index-tools-arxiv-0.3.0 llama-index-tools-wikipedia-0.3.1 primp-0.15.0 sgmllib3k-1.0.0 wikipedia-1.4.0\n",
            "Collecting llama-index-tools-brave-search\n",
            "  Downloading llama_index_tools_brave_search-0.3.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-tools-brave-search) (0.12.50)\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.11.15)\n",
            "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.21.0)\n",
            "Requirement already satisfied: banks<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.2.0)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.2.18)\n",
            "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.0.8)\n",
            "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.2.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2025.3.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.28.1)\n",
            "Requirement already satisfied: llama-index-workflows<2,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.1.0)\n",
            "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.5)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.0.2)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (11.2.1)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (4.3.8)\n",
            "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.11.7)\n",
            "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (80.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.0.41)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (8.5.0)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.9.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (4.14.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.17.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.20.1)\n",
            "Requirement already satisfied: griffe in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.1.6)\n",
            "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from llama-index-workflows<2,>=1.0.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (2025.7.14)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.2.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.1.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.26.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.16.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (25.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (1.3.1)\n",
            "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.11/dist-packages (from griffe->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->banks<3,>=2.0.0->llama-index-core<0.13.0,>=0.12.0->llama-index-tools-brave-search) (3.0.2)\n",
            "Downloading llama_index_tools_brave_search-0.3.0-py3-none-any.whl (2.8 kB)\n",
            "Installing collected packages: llama-index-tools-brave-search\n",
            "Successfully installed llama-index-tools-brave-search-0.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openaikey')"
      ],
      "metadata": {
        "id": "vobdEXgquH-I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QysSnlVpE3c-"
      },
      "source": [
        "And, let's log in to Hugging Face to use serverless Inference APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "POMls8o8E3c-"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "HUGGINGFACE_TOKEN = userdata.get('hugging')\n",
        "login(token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Simple RAG Systems"
      ],
      "metadata": {
        "id": "Wz3QdNYCtU7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Settings\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# Load document\n",
        "reader = SimpleDirectoryReader(input_files=[\"state.pdf\"])\n",
        "documents = reader.load_data()\n",
        "print(f\"Loaded {len(documents)} document(s).\")\n",
        "\n",
        "# Split into chunks\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "nodes = splitter.get_nodes_from_documents(documents)\n",
        "\n",
        "# Set up LLM and embedding model\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# Create vector index\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "# Create query engine\n",
        "query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUJReWButXJp",
        "outputId": "538a16aa-dae8-4783-ece1-a1bb43c38227"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 26 document(s).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1 Inspecting the vector store"
      ],
      "metadata": {
        "id": "hNDjd6WKAiGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the vector store data directly\n",
        "vector_store = vector_index.vector_store\n",
        "\n",
        "# Get embedding dictionary and node dictionary\n",
        "embedding_dict = vector_store.data.embedding_dict\n",
        "node_dict = vector_store.data.text_id_to_ref_doc_id\n",
        "\n",
        "print(f\"Number of embeddings: {len(embedding_dict)}\")\n",
        "print(f\"Number of node references: {len(node_dict)}\")\n",
        "\n",
        "# Show first few embeddings\n",
        "for i, (node_id, embedding) in enumerate(list(embedding_dict.items())[:3]):\n",
        "    print(f\"\\n--- Embedding {i} ---\")\n",
        "    print(f\"Node ID: {node_id}\")\n",
        "    print(f\"Embedding dimension: {len(embedding)}\")\n",
        "    print(f\"First 10 values: {embedding[:10]}\")"
      ],
      "metadata": {
        "id": "iipL_cCRAhAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4011951-5ea8-41df-c01c-5c9e83d43d73"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of embeddings: 27\n",
            "Number of node references: 27\n",
            "\n",
            "--- Embedding 0 ---\n",
            "Node ID: 0f82249b-499e-49f4-9c41-6c2b32116ba8\n",
            "Embedding dimension: 1536\n",
            "First 10 values: [-0.014158414676785469, -0.014000700786709785, -0.013491715304553509, -0.021520791575312614, 0.006523624062538147, 0.013068754225969315, -0.021406089887022972, 0.010093695484101772, -0.030051684007048607, -0.02446000650525093]\n",
            "\n",
            "--- Embedding 1 ---\n",
            "Node ID: 3a69f196-7730-46ce-b816-19bca19bc6e6\n",
            "Embedding dimension: 1536\n",
            "First 10 values: [-0.014881654642522335, -0.0216460432857275, -0.003096276894211769, -0.019051864743232727, -0.007670956198126078, 0.020209481939673424, -0.028438325971364975, 0.010843942873179913, -0.02125552110373974, -0.028424378484487534]\n",
            "\n",
            "--- Embedding 2 ---\n",
            "Node ID: 5537f8bd-56db-424b-bda0-9bfb8bc86b73\n",
            "Embedding dimension: 1536\n",
            "First 10 values: [0.0016024111537262797, -0.027652649208903313, -0.004749396815896034, -0.036062754690647125, -0.007328223902732134, 0.0169835165143013, -0.01657525822520256, 0.026659222319722176, -0.04079854488372803, -0.020099882036447525]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2 Asking questions to the RAG system"
      ],
      "metadata": {
        "id": "6caODl2LAkuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Query the document\n",
        "response = query_engine.query(\"Who is Lareina Yee?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQwyzi6StoOE",
        "outputId": "2db936a1-71e6-4588-f5db-0679534aed2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lareina Yee is a Senior partner and McKinsey Global Institute director.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3 Checking if the responses make sense"
      ],
      "metadata": {
        "id": "4FrcVI5_AoKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(response.source_nodes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzWW3i6d--cv",
        "outputId": "cf2167b3-95bc-4c3f-a275-d166ce344139"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out each source node\n",
        "print(\"Source nodes:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for i, node in enumerate(response.source_nodes):\n",
        "    print(f\"Node {i+1}:\")\n",
        "    print(f\"Score: {node.score}\")\n",
        "    print(f\"Text: {node.text}\")\n",
        "    print(f\"Metadata: {node.metadata}\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LSUGVGT1ACSL",
        "outputId": "a70bd902-ace7-4c2b-d032-8a6c70c23f79"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source nodes:\n",
            "==================================================\n",
            "Node 1:\n",
            "Score: 0.7229444954774485\n",
            "Text: The state of AI  \n",
            "March 2025\n",
            "Alex Singla  \n",
            "Alexander Sukharevsky  \n",
            "Lareina Yee  \n",
            "Michael Chui  \n",
            "Bryce Hall\n",
            "How organizations are rewiring to capture value\n",
            "Metadata: {'page_label': '1', 'file_name': 'state.pdf', 'file_path': 'state.pdf', 'file_type': 'application/pdf', 'file_size': 5564174, 'creation_date': '2025-07-19', 'last_modified_date': '2025-07-19'}\n",
            "------------------------------\n",
            "Node 2:\n",
            "Score: 0.7064591763562311\n",
            "Text: McKinsey commentary\n",
            "Lareina Yee\n",
            "Senior partner and McKinsey Global Institute director\n",
            "Although we remain in the early stages of gen AI, we’re beginning to get a glimpse into \n",
            "the ways the technology is affecting the workforce. A common fear about the technology \n",
            "is that it will be a job killer, as organizations offload tasks historically done by employees to \n",
            "increasingly powerful AI platforms. But our survey suggests that this is not necessarily the \n",
            "case. In fact, a plurality of respondents anticipate no immediate change to the size of their \n",
            "workforces. And while respondents expect lower head counts in some functions—such as \n",
            "service operations and supply chain/inventory management—in other functions—including \n",
            "software engineering and product development—respondents are actually anticipating an \n",
            "increase in the number of employees. \n",
            "Meantime, the difficulty of finding AI talent, while still considerable, is beginning to ease. \n",
            "Perhaps more people are taking the initiative to enhance their own capabilities. Or it could be \n",
            "that corporate investments in upskilling are beginning to bear fruit. Both of these somewhat \n",
            "counterintuitive trends serve to reinforce the fact that we are still in the early days of the AI \n",
            "revolution—the long-term workforce effects are still only beginning to take shape.   \n",
            "AI use continues to climb\n",
            " \n",
            "Reported use of AI increased in 2024.3 In the latest survey, 78 percent of respondents say \n",
            "their organizations use AI in at least one business function, up from 72 percent in early 2024 \n",
            "and 55 percent a year earlier (Exhibit 8). Respondents most often report using the technology \n",
            "in the IT and marketing and sales functions, followed by service operations. The business \n",
            "function that saw the largest increase in AI use in the past six months is IT, where the share of \n",
            "respondents reporting AI use jumped from 27 percent to 36 percent. \n",
            "Organizations are also using AI in more business functions than in the previous State of AI \n",
            "survey. For the first time, most survey respondents report the use of AI in more than one \n",
            "business function (Exhibit 9). Responses show organizations using AI in an average of three \n",
            "business functions—an increase from early 2024, but still a minority of functions.\n",
            "3 The survey question asked, “In which business functions has your organization adopted AI (for example, machine learning, \n",
            "computer vision, natural-language processing)?” Eleven business functions were offered as answer choices. Organizations \n",
            "using AI are those that, according to respondents, have adopted AI in at least one business function. For the purposes of our \n",
            "research, we left “adopted” undefined. Use of AI, therefore, spans from early experimentation by a few employees to AI being \n",
            "embedded across multiple business units that have entirely redesigned their business processes.\n",
            "14The state of AI: How organizations are rewiring to capture value\n",
            "Metadata: {'page_label': '15', 'file_name': 'state.pdf', 'file_path': 'state.pdf', 'file_type': 'application/pdf', 'file_size': 5564174, 'creation_date': '2025-07-19', 'last_modified_date': '2025-07-19'}\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask more questions\n",
        "response2 = query_engine.query(\"What are the main findings about AI adoption?\")\n",
        "print(response2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I59dl1cstpOY",
        "outputId": "b404ee27-9241-4ca4-bc47-6735cebc58be"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main findings about AI adoption include an increase in reported use of AI across organizations, with 78 percent of respondents stating their organizations use AI in at least one business function. The IT and marketing and sales functions are where AI is most commonly used, with IT seeing the largest increase in AI adoption. Additionally, organizations are now using AI in more business functions compared to previous surveys, with most respondents reporting AI use in more than one business function on average.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response3 = query_engine.query(\"What does the document say about AI risks?\")\n",
        "print(response3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu_T0jLStqU-",
        "outputId": "38738b8f-e62a-4dfe-b272-1a2c07ff05f0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The document highlights that organizations are actively working to mitigate various risks associated with AI, such as inaccuracy, cybersecurity, intellectual property infringement, privacy concerns, and other negative consequences related to the use of generative AI. It also mentions that larger organizations are more likely to address potential cybersecurity and privacy risks compared to other organizations, although they are not necessarily more focused on risks related to the accuracy or explainability of AI outputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1T80meUE3dA"
      },
      "source": [
        "## Part 2: Agentic RAG\n",
        "\n",
        "Let's now upgrade the previously defined RAG system into an Agentic RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fsJGdIJ3afAH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1: Loading the data"
      ],
      "metadata": {
        "id": "cAHJ_2iLt-gH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju5oAHyNX7du",
        "outputId": "7143158e-87ef-4b70-8e9b-dc94fc85d9e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 26 document(s).\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "reader = SimpleDirectoryReader(input_files=[\"state.pdf\"])\n",
        "documents = reader.load_data()\n",
        "\n",
        "print(f\"Loaded {len(documents)} document(s).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2: Breaking the data into chunks"
      ],
      "metadata": {
        "id": "9HWY21vPuBdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "\n",
        "# chunk_size of 1024 is a good default value\n",
        "splitter = SentenceSplitter(chunk_size=1024)\n",
        "# Create nodes from documents\n",
        "nodes = splitter.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "maOPpR4iis3M"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3 Define the LLM and the Embedding Model"
      ],
      "metadata": {
        "id": "CJnGnuAZuKzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import Settings\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "\n",
        "# LLM model\n",
        "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "# embedding model\n",
        "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-ada-002\")"
      ],
      "metadata": {
        "id": "3DCnVNCFizXV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Create the vector index and summary index"
      ],
      "metadata": {
        "id": "RZkCDa6UuOgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SummaryIndex, VectorStoreIndex\n",
        "\n",
        "# summary index\n",
        "summary_index = SummaryIndex(nodes)\n",
        "# vector store index\n",
        "vector_index = VectorStoreIndex(nodes)"
      ],
      "metadata": {
        "id": "aBu3TAWni2Vy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4 Create the vector query engine and summary query engine"
      ],
      "metadata": {
        "id": "YOUKB4heuSO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# summary query engine\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "\n",
        "# vector query engine\n",
        "vector_query_engine = vector_index.as_query_engine()"
      ],
      "metadata": {
        "id": "9l9lVrgNi6Dd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.5 Convert the vector and query engines into tools"
      ],
      "metadata": {
        "id": "YEpulllzuWHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=(\n",
        "        \"Useful for summarization questions related to the State of AI paper.\"\n",
        "    ),\n",
        ")\n",
        "\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=(\n",
        "        \"Useful for retrieving specific context from the the State of AI paper.\"\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "UHugM5x-i8Bf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.6 Define a superset query engine"
      ],
      "metadata": {
        "id": "ZYhHCqoLucC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import LLMSingleSelector\n",
        "\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=LLMSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        "    verbose=True\n",
        ")"
      ],
      "metadata": {
        "id": "LF3YTqDli-FR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.7 Test whether the query engine works"
      ],
      "metadata": {
        "id": "PKbSL_tbuflQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"Who is Lareina Yee according to teh document?\")\n",
        "print(str(response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAA2JIhai_n_",
        "outputId": "f066751a-a83d-417b-820e-47ede1f421bd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: Useful for retrieving specific context from the State of AI paper..\n",
            "\u001b[0mLareina Yee is one of the authors listed in the document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.8 Convert the query engine into a tool"
      ],
      "metadata": {
        "id": "vjBMU8pwul8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tool wrapper around router\n",
        "query_engine_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=query_engine,\n",
        "    name=\"state_of_ai_report_assistant\",\n",
        "    description=\"Answers questions based on the McKinsey 2025 State of AI report.\",\n",
        "    return_direct=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "-xAHwfwqulcO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.9 Define system prompt\n"
      ],
      "metadata": {
        "id": "lcy5m7bYuokd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a helpful assistant specialized in answering questions using the 'State of AI' March 2025 report by McKinsey.\n",
        "Your task is to:\n",
        "\n",
        "1. Use the Summary Tool when the user asks for high-level insights, trends, survey findings, or general understanding\n",
        "   (e.g., \"What are the top AI adoption practices?\" or \"Summarize the report's key findings\").\n",
        "\n",
        "2. Use the Vector Tool when the user is asking for specific statistics, organizational practices, exhibit-based\n",
        "   evidence, or detailed examples\n",
        "   (e.g., \"What percentage of companies track AI KPIs?\" or \"What are the risks companies are mitigating?\").\n",
        "\n",
        "Refer only to the content of the report. If the user's query is outside this context, politely decline or redirect.\n",
        "\n",
        "Check your answer multiple times to make sure it is actually relevant and mentioned in the document.\n",
        "\n",
        "Examples of summary queries:\n",
        "- \"How are companies restructuring to adopt GenAI?\"\n",
        "- \"What does the report say about workforce reskilling?\"\n",
        "\n",
        "Examples of specific/vector queries:\n",
        "- \"What percentage of companies have a roadmap for GenAI adoption?\"\n",
        "- \"Who is responsible for AI governance in large firms?\"\n",
        "\n",
        "Always explain clearly, referencing exact statistics, frameworks, or concepts when relevant. Be concise and insightful.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "adx0Y8UgmJHu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.10 Define the agent"
      ],
      "metadata": {
        "id": "b7xhEtfvut_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.agent.workflow import AgentWorkflow\n",
        "\n",
        "query_engine_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=[query_engine_tool],\n",
        "    llm=Settings.llm,\n",
        "    system_prompt=system_prompt,\n",
        ")\n"
      ],
      "metadata": {
        "id": "bt-1Pnr7jMfK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFUqiH8kE3dB"
      },
      "source": [
        "#### 2.11 Setup agent observability using Arize Phoenix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0jSmLawdeTEN"
      },
      "outputs": [],
      "source": [
        "# import llama_index\n",
        "# import os\n",
        "# from google.colab import userdata\n",
        "\n",
        "# PHOENIX_API_KEY = userdata.get('phoenix_api_key')\n",
        "# os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
        "# llama_index.core.set_global_handler(\n",
        "#     \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
        "# )\n",
        "\n",
        "\n",
        "# from phoenix.otel import register\n",
        "\n",
        "# tracer_provider = register(\n",
        "#   project_name=\"default\",\n",
        "#   endpoint=\"https://app.phoenix.arize.com/s/amank-iitg/v1/traces\",\n",
        "#   auto_instrument=True\n",
        "# )\n",
        "\n",
        "# import llama_index\n",
        "# import os\n",
        "# from google.colab import userdata\n",
        "\n",
        "# # Get the API key securely\n",
        "# PHOENIX_API_KEY = userdata.get('phoenix_api_key')\n",
        "\n",
        "# # Set the Authorization header correctly\n",
        "# os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Bearer {PHOENIX_API_KEY}\"\n",
        "\n",
        "# # Set llama_index handler (this is optional, only if you’re using it)\n",
        "# llama_index.core.set_global_handler(\n",
        "#     \"arize_phoenix\", endpoint=\"https://llamatrace.com/v1/traces\"\n",
        "# )\n",
        "\n",
        "# # Phoenix OTEL tracer setup\n",
        "# from phoenix.otel import register\n",
        "\n",
        "# tracer_provider = register(\n",
        "#   project_name=\"default\",\n",
        "#   endpoint=\"https://app.phoenix.arize.com/s/amank-iitg/v1/traces\",\n",
        "#   auto_instrument=True\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Get the API key\n",
        "PHOENIX_API_KEY = userdata.get('phoenix_api_key')\n",
        "if not PHOENIX_API_KEY:\n",
        "    raise ValueError(\"Phoenix API key is missing!\")\n",
        "\n",
        "# Set proper OTEL header\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"Authorization=Bearer {PHOENIX_API_KEY}\"\n",
        "\n",
        "# Register tracer\n",
        "from phoenix.otel import register\n",
        "\n",
        "tracer_provider = register(\n",
        "    project_name=\"default\",\n",
        "    endpoint=\"https://app.phoenix.arize.com/s/amank-iitg/v1/traces\",  # double check this!\n",
        "    auto_instrument=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcZyyXpJPtQ_",
        "outputId": "c7178db9-4b7d-4104-d3eb-b4a5a279c928"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔭 OpenTelemetry Tracing Details 🔭\n",
            "|  Phoenix Project: default\n",
            "|  Span Processor: SimpleSpanProcessor\n",
            "|  Collector Endpoint: https://app.phoenix.arize.com/s/amank-iitg/v1/traces\n",
            "|  Transport: HTTP + protobuf\n",
            "|  Transport Headers: {'authorization': '****'}\n",
            "|  \n",
            "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
            "|  \n",
            "|  ⚠️ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
            "|  \n",
            "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
            "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/phoenix/otel/otel.py:333: UserWarning: Could not infer collector endpoint protocol, defaulting to HTTP.\n",
            "  warnings.warn(\"Could not infer collector endpoint protocol, defaulting to HTTP.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.12 Run the agent and analyze responses"
      ],
      "metadata": {
        "id": "434C1PNgu2YT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxC0y5llE3dB",
        "outputId": "85696866-7270-48c7-c02f-16b2956706f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;3;38;5;200mSelecting query engine 1: The choice related to retrieving specific context from the State of AI paper would be more relevant for finding information about Yareina Lee..\n",
            "\u001b[0mLareina Yee is the Chief Diversity, Equity, and Inclusion Officer at McKinsey & Company. She is mentioned in the \"Acknowledgments\" section of the report, where she is credited for her contributions to the research and insights provided in the document.\n"
          ]
        }
      ],
      "source": [
        "# In Jupyter/Colab, you can use await directly\n",
        "question = \"Who is Yareina Lee according to the document? Where is she mentioned in the document and in what context?\"\n",
        "response = await query_engine_agent.run(question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.13 Equip the agent with multiple tools"
      ],
      "metadata": {
        "id": "1PxekjRYu5HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "print(\"PHOENIX_API_KEY:\", PHOENIX_API_KEY)\n",
        "print(\"OTEL_EXPORTER_OTLP_HEADERS:\", os.environ.get(\"OTEL_EXPORTER_OTLP_HEADERS\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fkBji-joOLP",
        "outputId": "fd3fe923-b3ab-4639-8e34-1dbf2fff5ebd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PHOENIX_API_KEY: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiJBcGlLZXk6MSJ9.Z3zRiBSq5IS2BBzU5CB1VI-pRipGmad8zgA8lND8fqY\n",
            "OTEL_EXPORTER_OTLP_HEADERS: Authorization=Bearer eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJqdGkiOiJBcGlLZXk6MSJ9.Z3zRiBSq5IS2BBzU5CB1VI-pRipGmad8zgA8lND8fqY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.14 Add the new tools (ArXiV, Brave Search)\n"
      ],
      "metadata": {
        "id": "16E17tfjvLIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import additional tools\n",
        "from llama_index.tools.arxiv import ArxivToolSpec\n",
        "from llama_index.tools.wikipedia import WikipediaToolSpec\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "from llama_index.tools.brave_search import BraveSearchToolSpec\n",
        "\n",
        "import requests\n",
        "import json\n"
      ],
      "metadata": {
        "id": "soxyYOVfoAHB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create ArXiV tool\n",
        "\n",
        "arxiv_tool = ArxivToolSpec()\n",
        "\n",
        "arxiv_tools = arxiv_tool.to_tool_list()\n",
        "\n",
        "\n",
        "# Create Brave Search tool\n",
        "\n",
        "brave_search_tool_spec = BraveSearchToolSpec(api_key=userdata.get('braveapikey'))\n",
        "brave_search_tools = brave_search_tool_spec.to_tool_list()\n"
      ],
      "metadata": {
        "id": "m2lC5H1SoYpm"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create enhanced agent with multiple tools - FIX: Use extend instead of append\n",
        "enhanced_tools = [query_engine_tool]  # Start with McKinsey report tool\n",
        "enhanced_tools.extend(brave_search_tools)  # Add all brave search tools\n",
        "enhanced_tools.extend(arxiv_tools)  # Add all arxiv tools"
      ],
      "metadata": {
        "id": "Rp0WSiqpodii"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.15 Define the enhanced agent with all tools\n"
      ],
      "metadata": {
        "id": "RjWiEqbHvSmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create new enhanced agent\n",
        "enhanced_agent = AgentWorkflow.from_tools_or_functions(\n",
        "    tools_or_functions=enhanced_tools,\n",
        "    llm=Settings.llm,\n",
        "    system_prompt=\"\"\"You are an AI research assistant with access to:\n",
        "    1. The McKinsey 2025 State of AI report\n",
        "    2. Web search capabilities\n",
        "    3. ArXiv research paper search\n",
        "\n",
        "    Use these tools to provide comprehensive, well-researched answers. When discussing AI trends,\n",
        "    combine insights from the McKinsey report with recent research and web findings.\"\"\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "QFaTmUITooBF"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.16 Battle test agent with multiple questions!\n"
      ],
      "metadata": {
        "id": "31wclWR0vWVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test questions that can benefit from multiple tools\n",
        "\n",
        "# Question 1: Combine McKinsey insights with recent research\n",
        "question1 = \"\"\"According to the McKinsey report, what are the main organizational changes companies are making for AI adoption?\n",
        "Can you also search for recent research papers on AI governance and organizational transformation to provide additional context?\"\"\"\n",
        "\n",
        "print(\"Question 1: Organizational changes and governance\")\n",
        "print(\"=\" * 50)\n",
        "response1 = await enhanced_agent.run(question1)\n",
        "print(response1)\n",
        "print(\"\\n\" + \"=\"*80 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvuCQ2GAoqmQ",
        "outputId": "b9365952-ff43-43c5-aa47-8e4fe28294ef"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: Organizational changes and governance\n",
            "==================================================\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: Summarization questions related to the State of AI paper would likely cover the main organizational changes companies are making for AI adoption..\n",
            "\u001b[0m### Recent Research Papers on AI Governance:\n",
            "1. **[AI Governance: Themes, Knowledge Gaps, and Future Agendas](https://www.emerald.com/insight/content/doi/10.1108/intr-01-2022-0042/full/html)** - This paper synthesizes the organizational AI governance literature by outlining research themes and addressing the demand for translating ethical principles into practice through AI governance.\n",
            "\n",
            "2. **[AI Governance: A Systematic Literature Review](https://link.springer.com/article/10.1007/s43681-024-00653-w)** - This review presents the state of the art on AI Governance by filling gaps in the literature and extracting relevant information on who governs AI and stakeholders involved.\n",
            "\n",
            "3. **[Responsible Artificial Intelligence Governance: A Review and Research Framework](https://www.sciencedirect.com/science/article/pii/S0963868724000672)** - This paper defines responsible AI governance based on critical reflection, develops a research agenda, and identifies key areas for future research within the IS domain.\n",
            "\n",
            "### Recent Research Papers on Organizational Transformation and AI:\n",
            "Unfortunately, due to a limitation in accessing the search results for recent research papers on organizational transformation and AI, I couldn't retrieve specific papers at this moment. If you would like, I can attempt the search again or provide information on a different topic.\n",
            "\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2: Workflow Redesign and Implementation\n",
        "question2 = \"\"\"What does the McKinsey report say about workflow redesign for AI implementation?\n",
        "Search ArXiv for papers on business process automation with AI and find current web articles about workflow transformation.\"\"\"\n",
        "\n",
        "print(\"Question 2: Workflow Redesign\")\n",
        "response2 = await enhanced_agent.run(question2)\n",
        "print(response2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-99uMZimpCXj",
        "outputId": "ad8d728e-bc16-48e1-df77-0a363081c4e9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 2: Workflow Redesign\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: Workflow redesign for AI implementation may involve summarizing key points and findings from the State of AI paper to inform decision-making and strategy..\n",
            "\u001b[0m### ArXiv Paper on Business Process Automation with AI:\n",
            "Unfortunately, the search for papers on business process automation with AI on ArXiv did not return any results. \n",
            "\n",
            "### Web Articles on Workflow Transformation:\n",
            "1. **[How Workflow Automation Delivers Business Transformation](https://www.flowforma.com/blog/how-workflow-automation-delivers-business-transformation)**\n",
            "   - This article discusses workflow automation and how it delivers business transformation by making processes more intuitive, efficient, and effective.\n",
            "\n",
            "2. **[Workflow Transformation | Buddha Logic](https://buddhalogic.com/solutions/workflow-transformation/)**\n",
            "   - Buddha Logic explains workflow transformation as the process of examining content-focused processes and designing advanced software solutions to enhance efficiency.\n",
            "\n",
            "3. **[Digital Workflow Transformation | Business Process Automation | Kofax](https://www.tungstenautomation.com/workflows)**\n",
            "   - Tungsten Automation offers end-to-end digital business workflow transformation solutions, automating processes to manage and scale for the future.\n",
            "\n",
            "4. **[Workflow > Data Transformation](https://support.medit.com/hc/en-us/articles/360052595291--Workflow-Data-Transformation)**\n",
            "   - This article by Medit discusses data transformation within workflows, allowing users to adjust data size and scale for various applications.\n",
            "\n",
            "5. **[Why Digital Transformation Starts with Workflows | IBM](https://www.ibm.com/think/insights/why-digital-transformation-starts-with-workflows-that-unite-people-and-technology)**\n",
            "   - IBM emphasizes the importance of workflows that unite people and technology in digital transformation, enabling seamless global trade and rapid deployment.\n",
            "\n",
            "### Web Videos on Workflow Transformation:\n",
            "1. **[Optimize Your Workflow: Digital Transformation with IMAGINiT - YouTube](https://www.youtube.com/watch?v=KuDFmdZVHsY)**\n",
            "   - IMAGINiT Technologies specializes in engineering digital transformation solutions to optimize workflows.\n",
            "\n",
            "2. **[Generative AI can transform your workflow. - YouTube](https://www.youtube.com/watch?v=eXa3IJAhrgc)**\n",
            "   - The FP&A Guy discusses how generative AI can transform workflows in finance.\n",
            "\n",
            "3. **[Don't Fear Workflow Transformation | #ExpertAdvice from ... - YouTube](https://www.youtube.com/watch?v=YWthy5jSPXk)**\n",
            "   - Solutions Review features Jeremy Grue, a Sales Engineer at HighGear, providing expert advice on workflow transformation.\n",
            "\n",
            "4. **[Digital Workflow Transformation - YouTube](https://www.youtube.com/watch?v=_XhV1b6a3yU)**\n",
            "   - This video showcases digital workflow transformation and its impact on processes.\n",
            "\n",
            "5. **[Scanning Workflow Transformation: Global Objects Case Study - YouTube](https://www.youtube.com/watch?v=lPO1J7irUqY)**\n",
            "   - MeshInspector App & SDK presents a case study on how MeshInspector revolutionized 3D scanning workflows.\n",
            "\n",
            "These resources provide insights into workflow transformation, automation, and the integration of AI in business processes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3: Risk management and future trends\n",
        "question3 = \"\"\"Based on the McKinsey report, what are the key risks organizations are addressing with gen AI?\n",
        "Can you search the web for recent academic research on AI risk mitigation and compare with the report's findings?\"\"\"\n",
        "\n",
        "print(\"Question 3: Risk management\")\n",
        "response3 = await enhanced_agent.run(question3)\n",
        "print(response3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieLf-4v8qB8C",
        "outputId": "dba75a7c-fbe1-417a-ff07-bd27f97e3c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 3: Risk management\n",
            "\u001b[1;3;38;5;200mSelecting query engine 0: The question is asking for a summary of the key risks organizations are addressing with gen AI, which is more aligned with summarization questions related to the State of AI paper..\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question that forces usage of all three tools\n",
        "comprehensive_question = \"\"\"Who is Lareina Yee in the McKinsey document and what are her views on AI's workforce impact?\n",
        "\n",
        "After finding information about her from the document, please:\n",
        "1. Search the web using Brave Search for recent articles, interviews, or news about Lareina Yee and her work on AI\n",
        "2. Search ArXiv for any research papers she may have authored or co-authored related to AI, workforce transformation, or economic impact\n",
        "3. Provide a comprehensive profile combining insights from all three sources about her expertise and contributions to AI research\"\"\"\n",
        "\n",
        "print(\"Question: Comprehensive profile of Lareina Yee\")\n",
        "print(\"=\" * 60)\n",
        "print(\"This question should force the agent to use:\")\n",
        "print(\"1. Query Engine - to find info about Lareina Yee in the McKinsey document\")\n",
        "print(\"2. Brave Search - to find recent web articles/news about her\")\n",
        "print(\"3. ArXiv Search - to find any academic papers she's authored\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "response = await enhanced_agent.run(comprehensive_question)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "zUIlpdi8qB5_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-M4zgrIYtzEW",
        "Wz3QdNYCtU7u"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}